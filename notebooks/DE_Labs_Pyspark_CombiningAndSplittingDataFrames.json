{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** Learn how to combine and split DataFrames\n**File locations:** s3://shared-edu/sci/\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Working with DataFrames\n\nCopyright © 2010–2021 Cloudera. All rights reserved.\nNot to be reproduced or shared without prior written consent from Cloudera.\n\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:35:35+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360715_647690652","id":"20171105-200834_1116095891","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:11+0000","dateFinished":"2022-01-24T16:32:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:74593"},{"text":"%md\n## Overview\n\nIn this module we demonstrate how to combine and split DataFrames.\n\n## Combining and Splitting DataFrames\n\n* Spark SQL supports the usual database-style joins:\n    * Cross join\n    * Inner join\n    * Left semi join\n    * Left anti join\n    * Left outer join\n    * Right outer join\n    * Full outer join\n\n* Joins are expensive in the big-data world\n    * Perform joins early in the process\n    * Amortize the cost over many use cases\n\n* Spark SQL supports the following set operations:\n    * Union\n    * Intersection\n    * Subtraction\n\n* Spark SQL provides a method to split a DataFrame into random subsets","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360716_-3893011","id":"20210609-222433_1574519092","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:12+0000","dateFinished":"2022-01-24T16:32:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74594"},{"text":"%md\n# Setup\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360717_-1868325711","id":"20181114-164229_902436001","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:13+0000","dateFinished":"2022-01-24T16:32:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74595"},{"title":"Required for Kerberos operations","text":"%sh\n\necho $PASS | kinit $USER","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:36:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040543080_-994927236","id":"20220124-160903_800712102","dateCreated":"2022-01-24T16:09:03+0000","dateStarted":"2022-01-24T16:32:14+0000","dateFinished":"2022-01-24T16:32:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74596"},{"title":"Setup the data context","text":"%sh\n\nrm -rf data\nmkdir -p data\naws s3 cp --recursive s3://shared-edu/sci/duocar/raw data/raw\naws s3 cp --recursive s3://shared-edu/sci/duocar/clean data/clean\naws s3 cp --recursive s3://shared-edu/sci/duocar/joined data/joined\nhdfs dfs -rm -r duocar\nhdfs dfs -mkdir -p duocar\nhdfs dfs -put -f data duocar","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:36:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040563196_-507853533","id":"20220124-160923_2113418172","dateCreated":"2022-01-24T16:09:23+0000","dateStarted":"2022-01-24T16:32:15+0000","dateFinished":"2022-01-24T16:32:30+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74597"},{"text":"%md\n# Lesson\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360717_1830423419","id":"20210609-150445_381117564","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:31+0000","dateFinished":"2022-01-24T16:32:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74598"},{"text":"%md\n## Joining DataFrames","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360718_676324976","id":"20210609-231102_1173405065","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:32+0000","dateFinished":"2022-01-24T16:32:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74599"},{"title":"We will use the following DataFrames to demonstrate joins","text":"%pyspark\n\nscientists = spark.read.csv(\"duocar/data/raw/data_scientists/\", header=True, inferSchema=True)\nscientists.show()\n\noffices = spark.read.csv(\"duocar/data/raw/offices/\", header=True, inferSchema=True)\noffices.show()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:32+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360718_244793198","id":"20210609-231136_1486159156","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:33+0000","dateFinished":"2022-01-24T16:32:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74600"},{"title":"Cross join","text":"%pyspark\n\n# Use the `crossJoin` DataFrame method to join every row in the left (`scientists`) DataFrame with every row in the right (`offices`) DataFrame:\nscientists.crossJoin(offices).show()\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:34+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360719_543141227","id":"20210609-231416_373015015","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:35+0000","dateFinished":"2022-01-24T16:32:36+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74601"},{"title":"Warning","text":"%md\nThis can result in very big DataFrames!","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360719_1550367149","id":"20210609-231614_1055557420","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:37+0000","dateFinished":"2022-01-24T16:32:37+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74602"},{"title":"Note","text":"%md\nColumns with the same name are not renamed.\nThis is called the *Cartesian product* of the two DataFrames.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360719_-1067072876","id":"20210609-231636_1254576650","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:38+0000","dateFinished":"2022-01-24T16:32:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74603"},{"title":"Inner join","text":"%pyspark\n\n# Use the `join` DataFrame method with different values of the `how` argument to perform other types of joins\n\n# Use a join expression and the value `inner` to return only those rows for which the join expression is true:\nscientists.join(offices, scientists.office_id == offices.office_id, \"inner\").show()\n\n# This gives us a list of data scientists associated with an office and the corresponding office information.\n\n# Since the join key has the same name on both DataFrames, we can simplify the join as follows:\nscientists.join(offices, \"office_id\", \"inner\").show()\n\n# Since an inner join is the default, we can further simplify the join as follows:\nscientists.join(offices, \"office_id\").show()\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:38+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360720_1567303102","id":"20210609-231821_1865619957","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:39+0000","dateFinished":"2022-01-24T16:32:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74604"},{"title":"Left semi join","text":"%pyspark\n\n# Use the value `left_semi` to return the rows in the left DataFrame that match rows in the right DataFrame:\nscientists \\\n  .join(offices, scientists.office_id == offices.office_id, \"left_semi\") \\\n  .show()\n\n# This gives us a list of data scientists associated with an office.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360720_1454845209","id":"20210609-232653_529939127","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:41+0000","dateFinished":"2022-01-24T16:32:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74605"},{"title":"Left anti join","text":"%pyspark\n\n# Use the value `left_anti` to return the rows in the left DataFrame that do not match rows in the right DataFrame:\nscientists \\\n  .join(offices, scientists.office_id == offices.office_id, \"left_anti\") \\\n  .show()\n\n# This gives us a list of data scientists not associated with an office.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360720_700463430","id":"20210609-232830_1046047309","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:43+0000","dateFinished":"2022-01-24T16:32:44+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74606"},{"title":"Note","text":"%md\nYou can think of the left semi and left anti joins as special types of filters.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360721_752412566","id":"20210609-232929_1918392793","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:45+0000","dateFinished":"2022-01-24T16:32:45+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74607"},{"title":"Left outer join","text":"%pyspark\n\n# Use the value `left` or `left_outer` to return every row in the left DataFrame with or without matching rows in the right DataFrame:\nscientists \\\n  .join(offices, scientists.office_id == offices.office_id, \"left_outer\") \\\n  .show()\n\n# This gives us a list of data scientists with or without an office.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:45+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360721_437988958","id":"20210609-232928_637649899","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:46+0000","dateFinished":"2022-01-24T16:32:47+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74608"},{"title":"Right outer join","text":"%pyspark\n\n# Use the value `right` or `right_outer` to return every row in the right\n# DataFrame with or without matching rows in the left DataFrame:\nscientists \\\n  .join(offices, scientists.office_id == offices.office_id, \"right_outer\") \\\n  .show()\n\n# This gives us a list of offices with or without a data scientist.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:47+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360722_703994470","id":"20210609-233132_835639263","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:48+0000","dateFinished":"2022-01-24T16:32:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74609"},{"title":"Note","text":"%md\nThe Paris office has two data scientists.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360722_-971053632","id":"20210609-233310_197367113","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:50+0000","dateFinished":"2022-01-24T16:32:50+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74610"},{"title":"Full outer join","text":"%pyspark\n\n# Use the value `full`, `outer`, or `full_outer` to return the union of the left outer and right outer joins (with duplicates removed):\nscientists \\\n  .join(offices, scientists.office_id == offices.office_id, \"full_outer\") \\\n  .show()\n\n# This gives us a list of all data scientists whether or not they have an office and all offices whether or not they have any data scientists.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:50+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360722_-278549798","id":"20210609-233410_1919989150","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:51+0000","dateFinished":"2022-01-24T16:32:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74611"},{"text":"%md\n### Example: Joining the DuoCar data\n\nLet us join the driver, rider, and review data with the ride data.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360723_1904272688","id":"20210609-233554_1146424900","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:53+0000","dateFinished":"2022-01-24T16:32:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74612"},{"text":"%sh\nhdfs dfs -ls duocar/data/clean/rides","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643041389922_-950325855","id":"20220124-162309_70229501","dateCreated":"2022-01-24T16:23:09+0000","dateStarted":"2022-01-24T16:32:54+0000","dateFinished":"2022-01-24T16:32:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74613"},{"title":"Read the clean data from HDFS","text":"%pyspark\n\nrides = spark.read.parquet(\"duocar/data/clean/rides/\")\ndrivers = spark.read.parquet(\"duocar/data/clean/drivers/\")\nriders = spark.read.parquet(\"duocar/data/clean/riders/\")\nreviews = spark.read.parquet(\"duocar/data/clean/ride_reviews/\")","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360723_693626102","id":"20210609-233639_1852715423","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:57+0000","dateFinished":"2022-01-24T16:32:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74614"},{"title":"Since we want all the ride data, we will use a sequence of left outer joins","text":"%pyspark\n\njoined = rides \\\n  .join(drivers, rides.driver_id == drivers.id, \"left_outer\") \\\n  .join(riders, rides.rider_id == riders.id, \"left_outer\") \\\n  .join(reviews, rides.id == reviews.ride_id, \"left_outer\")\njoined.printSchema()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:32:58+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360724_1741362758","id":"20210609-233827_952416184","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:32:59+0000","dateFinished":"2022-01-24T16:33:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74615"},{"title":"Note","text":"%md\nWe probably want to rename some columns before joining the data and remove the duplicate ID columns after joining the data to make this DataFrame  more usable.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360724_1819823770","id":"20210609-233930_2039257022","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:01+0000","dateFinished":"2022-01-24T16:33:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74616"},{"title":"For example, see the `joined` data in the DuoCar data repository","text":"%pyspark\n\nspark.read.parquet(\"duocar/data/joined/\").printSchema()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:01+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360724_-1620369230","id":"20210609-233913_91627979","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:02+0000","dateFinished":"2022-01-24T16:33:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74617"},{"text":"%md\n\n## Applying set operations to DataFrames\n\nSpark SQL provides the following DataFrame methods that implement set operations:\n* `union`\n* `intersect`\n* `subtract`","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360725_-1590019837","id":"20210609-234159_1381331125","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:04+0000","dateFinished":"2022-01-24T16:33:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74618"},{"title":" Use the `union` method to get the union of rows in two DataFrames with similar schema","text":"%pyspark\n\ndriver_names = drivers.select(\"first_name\")\ndriver_names.count()\n\nrider_names = riders.select(\"first_name\")\nrider_names.count()\n\nnames_union = driver_names.union(rider_names).orderBy(\"first_name\")\nnames_union.count()\nnames_union.show()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:04+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360725_-819562513","id":"20210609-234321_1548059875","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:05+0000","dateFinished":"2022-01-24T16:33:07+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74619"},{"title":"Note that `union` does not remove duplicates. Use the `distinct` method to remove duplicates","text":"%pyspark\n\nnames_distinct = names_union.distinct()\nnames_distinct.count()\nnames_distinct.show()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:07+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360726_-1873505904","id":"20210609-234423_2092462479","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:08+0000","dateFinished":"2022-01-24T16:33:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74620"},{"title":"Use the `intersect` method to return rows that exist in both DataFrames","text":"%pyspark\n\nname_intersect = driver_names.intersect(rider_names).orderBy(\"first_name\")\nname_intersect.count()\nname_intersect.show()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:10+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360726_-83816492","id":"20210609-234548_1163824754","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:11+0000","dateFinished":"2022-01-24T16:33:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74621"},{"title":"Use the `subtract` method to return rows in the left DataFrame that do not exist in the right DataFrame","text":"%pyspark\n\nnames_subtract = driver_names.subtract(rider_names).orderBy(\"first_name\")\nnames_subtract.count()\nnames_subtract.show()","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:14+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360727_-2120225583","id":"20210609-234546_925355223","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:15+0000","dateFinished":"2022-01-24T16:33:18+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74622"},{"text":"%md\n ## Splitting a DataFrame","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360727_-1891854361","id":"20210609-234757_657885115","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:19+0000","dateFinished":"2022-01-24T16:33:19+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74623"},{"title":"Use the `randomSplit` DataFrame method to split a DataFrame into random subsets","text":"%pyspark\n\nriders.count()\n(train, validate, test) = riders.randomSplit(weights=[0.6, 0.2, 0.2])\n(train.count(), validate.count(), test.count())\n\n# Use the `seed` argument to ensure replicability:\n(train, validate, test) = riders.randomSplit([0.6, 0.2, 0.2], seed=12345)\n(train.count(), validate.count(), test.count())\n\n# If the proportions do not add up to one, then Spark will normalize the values:\n(train, validate, test) = riders.randomSplit([60.0, 20.0, 20.0], seed=12345)\n(train.count(), validate.count(), test.count())","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:19+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360728_691667476","id":"20210609-234752_1390806088","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:20+0000","dateFinished":"2022-01-24T16:33:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74624"},{"title":"Note","text":"%md\nThe weights must be doubles.\nThe same seed will result in the same random split.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360729_1126939396","id":"20210609-234752_348698865","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:23+0000","dateFinished":"2022-01-24T16:33:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74625"},{"text":"%md\n# Lab\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360729_-1870437847","id":"20210609-150527_79851234","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:24+0000","dateFinished":"2022-01-24T16:33:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74626"},{"title":"1 - Join the `rides` DataFrame with the `reviews` DataFrame.  Keep only those rides that have a review","text":"%pyspark\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360730_2074617373","id":"20210609-235614_801953798","dateCreated":"2022-01-24T16:06:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74627"},{"title":"2 - How many drivers have not provided a ride?","text":"%pyspark\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:26+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360731_-1194705466","id":"20210609-235732_713641342","dateCreated":"2022-01-24T16:06:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74628"},{"text":"%md\n# Result\n**You have now:** learned how to combine and split DataFrames.\n\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360731_-1375440383","id":"20181119-142716_792318228","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:27+0000","dateFinished":"2022-01-24T16:33:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74629"},{"text":"%md\n# Solution\n---","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360732_1437155989","id":"20171113-155535_1769142099","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:28+0000","dateFinished":"2022-01-24T16:33:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74630"},{"title":"1 - Join the `rides` DataFrame with the `reviews` DataFrame.  Keep only those rides that have a review","text":"%pyspark\n\n# Count the number of reviews before the join:\nreviews.count()\n\n# Perform a right outer join:\nrides_with_reviews = rides.join(reviews, rides.id == reviews.ride_id, \"right_outer\")\n\n# Count the number of reviews after the join:\nrides_with_reviews.count()\n\n# Print the schema:\nrides_with_reviews.printSchema()\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:28+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360733_1811210813","id":"20210609-235838_715916045","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:29+0000","dateFinished":"2022-01-24T16:33:30+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74631"},{"title":"2 - How many drivers have not provided a ride?","text":"%pyspark\n\n# Get the driver IDs from `drivers` DataFrame:\nid_from_drivers = drivers.select(\"id\")\n\n# Get the driver IDs from `rides` DataFrame:\nid_from_rides = rides.select(\"driver_id\").withColumnRenamed(\"driver_id\", \"id\")\n\n# Find lazy drivers using a left anti join:\nlazy_drivers1 = id_from_drivers.join(id_from_rides, \"id\", \"left_anti\")\nlazy_drivers1.count()\nlazy_drivers1.orderBy(\"id\").show(5)\n\n# Find lazy drivers using a subtraction:\nlazy_drivers2 = id_from_drivers.subtract(id_from_rides)\nlazy_drivers2.count()\nlazy_drivers2.orderBy(\"id\").show(5)","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:30+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360733_-2111429973","id":"20210609-235831_219349651","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:31+0000","dateFinished":"2022-01-24T16:33:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74632"},{"title":"References","text":"%md\n\n* [Spark Python API - crossJoin DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.crossJoin.html#pyspark.sql.DataFrame.crossJoin)\n\n* [Spark Python API - join DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join)\n\n* [Spark Python API - union DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html#pyspark.sql.DataFrame.union)\n\n* [Spark Python API - intersect DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.intersect.html#pyspark.sql.DataFrame.intersect)\n\n* [Spark Python API - subtract DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.subtract.html#pyspark.sql.DataFrame.subtract)\n\n* [Spark Python API - randomSplit DataFrame method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html#pyspark.sql.DataFrame.randomSplit)\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360735_-1094689025","id":"20210609-235023_710325981","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:36+0000","dateFinished":"2022-01-24T16:33:36+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74633"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360736_2087093567","id":"20181116-135131_93712280","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:38+0000","dateFinished":"2022-01-24T16:33:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74634"},{"text":"%angular\n</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera University\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:38+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360737_-2084258812","id":"20200110-154537_1406191376","dateCreated":"2022-01-24T16:06:00+0000","dateStarted":"2022-01-24T16:33:40+0000","dateFinished":"2022-01-24T16:33:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74635"},{"text":"%angular\n","user":"freynalddeveuwest1","dateUpdated":"2022-01-24T16:33:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1643040360738_2130500747","id":"20200110-162013_302547143","dateCreated":"2022-01-24T16:06:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74636"}],"name":"DE/Labs/Pyspark/CombiningAndSplittingDataFrames","id":"2GVZ62RUR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}